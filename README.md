# Plugin de Panel para Grafana: Interpretaci√≥n con LLM Local

> **Una IA que te ayuda a ver m√°s all√° de los gr√°ficos**

Este proyecto integra **modelos de lenguaje multimodales locales** (como Llama 3.2 Vision, GPT-4o Vision o Moondream) con Grafana para interpretar visualizaciones autom√°ticamente y convertir informaci√≥n visual en explicaciones claras y √∫tiles.

**¬øEl resultado?** Un dashboard que no solo muestra datos, sino que tambi√©n los explica, detecta anomal√≠as y sugiere posibles causas o acciones.

Este repositorio es un fork del desarrollado por Tom Glenn en este [enlace](https://github.com/tomglenn/tomglenn-openaianalyser-panel).

---

## üéØ ¬øPor qu√© usar una LLM local en Grafana?

Los plugins son paneles que permiten agregar nuevos tipos de visualizaciones o interacciones en un dashboard de Grafana. En este caso, el objetivo es permitir que un modelo de lenguaje interprete los gr√°ficos y genere descripciones autom√°ticas √∫tiles para el usuario.

Imagin√° que ten√©s un dashboard con m√∫ltiples m√©tricas y detect√°s una anomal√≠a en un gr√°fico. Una LLM puede:


- **Interpretar tendencias** en tiempo real
- **Detectar anomal√≠as** o picos autom√°ticamente  
- **Sugerir causas probables** de comportamientos observados
- **Explicar m√©tricas** a personas no t√©cnicas
- **Comparar per√≠odos hist√≥ricos** similares
- **Mantener la privacidad** de tus datos (todo local, sin APIs externas)

---

## üîÑ ¬øC√≥mo funciona?

El proceso ensambla dos mundos: **visualizaci√≥n** y **IA Generativa**:

1. **Captura de datos del gr√°fico**: Se extraen los datos relevantes del panel de Grafana
2. **Procesamiento por LLM**: La informaci√≥n se env√≠a a un modelo de lenguaje local (via Ollama)
3. **Generaci√≥n de insights**: La LLM devuelve an√°lisis/res√∫menes en lenguaje natural
4. **Visualizaci√≥n integrada**: Los resultados se presentan junto al gr√°fico original

---

## üìã Requisitos previos

### Node.js
Necesit√°s **Node.js** para el desarrollo del plugin. Recomendamos usar `nvm`:

```bash
# Descargar e instalar nvm:
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash

# (Opcional) Cargar nvm sin reiniciar la terminal:
. "$HOME/.nvm/nvm.sh"

# Instalar Node.js versi√≥n 22:
nvm install 22
```

### Verificar instalaci√≥n:
```bash
node -v       # "v22.14.0"
nvm current   # "v22.14.0"
npm -v        # "10.9.2"
```

### Ollama (para LLM local)
```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Descargar modelo (ejemplo con Moondream - 1.8B par√°metros)
ollama pull moondream

# O usar un modelo m√°s potente como Llama 3.2 Vision
ollama pull llama3.2-vision
```

---

## üöÄ Configuraci√≥n paso a paso

### 1. Clonar y levantar el entorno

```bash
git clone [tu-repo]
cd grafana-plugin-local

# Levantar Grafana + TimescaleDB
docker-compose up -d
```

Grafana estar√° disponible en: http://localhost:3000

### 2. Poblar la base de datos con datos financieros

Ten√©s dos opciones para obtener datos del S&P 500:

**Opci√≥n A: API de Yahoo Finance (puede tener rate limiting)**
```bash
poetry run python upload/download.py
```

**Opci√≥n B: Ingesti√≥n local desde CSV (recomendado)**
```bash
poetry run python data_ingestion/local_ingest.py
```

### 3. Instalar el plugin

```bash
# Instalar dependencias
npm install

# Ejecutar en modo desarrollo
npm run dev
```

Si hay problemas de permisos:
```bash
sudo chown -R $USER:$USER .
```

### 4. Reiniciar containers
```bash
docker-compose restart
```

---

## üìä Uso del plugin

Una vez instalado, ver√°s el panel "Chart Analyzer" en la esquina superior derecha de tus dashboards.

### Tipos de an√°lisis disponibles:

- **üîç An√°lisis de tendencias**: Detecta patrones crecientes o decrecientes
- **‚ö†Ô∏è Detecci√≥n de anomal√≠as**: Identifica picos o valores at√≠picos  
- **üìà Comparativa hist√≥rica**: Contrasta per√≠odos similares
- **üí° Explicaci√≥n para no t√©cnicos**: Traduce m√©tricas a lenguaje simple

### Configuraci√≥n del plugin:

1. Hace clic en los **tres puntos** del panel
2. Selecciona **"Edit"**
3. Configura:
   - **URL de la LLM**: Por defecto `http://localhost:11434` (Ollama)
   - **Modelo**: `moondream`, `llama3.2-vision`, etc.
   - **Prompts personalizados**: Edit√° los prompts seg√∫n tus necesidades

---

## üõ†Ô∏è Desarrollo

### Estructura del proyecto:
```
src/
‚îú‚îÄ‚îÄ components/          # Componentes React del panel
‚îú‚îÄ‚îÄ img/                # Recursos gr√°ficos  
‚îú‚îÄ‚îÄ module.ts           # Configuraci√≥n del plugin
‚îú‚îÄ‚îÄ plugin.json         # Metadatos del plugin
‚îî‚îÄ‚îÄ types.ts            # Definiciones de tipos

data_ingestion/         # Scripts para poblar datos
‚îú‚îÄ‚îÄ download.py         # Descarga desde Yahoo Finance
‚îî‚îÄ‚îÄ local_ingest.py     # Ingesta desde CSV local

provisioning/           # Configuraci√≥n de Grafana
‚îú‚îÄ‚îÄ dashboards/         # Dashboards predefinidos
‚îî‚îÄ‚îÄ datasources/        # Configuraci√≥n de TimescaleDB
```

### Comandos √∫tiles:

```bash
# Desarrollo con hot-reload
npm run dev

# Build para producci√≥n
npm run build

# Linting
npm run lint

# Tests
npm run test
```

---

## üîí Ventajas de la implementaci√≥n local

- **Privacidad**: Tus datos nunca salen de tu infraestructura
- **Velocidad**: Sin latencia de APIs externas
- **Control total**: Eleg√≠s el modelo y configuraci√≥n que mejor se adapte
- **Sin costos por API**: Una vez configurado, no hay l√≠mites de uso
- **Personalizaci√≥n**: Prompts y comportamiento completamente customizables

---

## üì¶ Distribuci√≥n

Para distribuir el plugin (p√∫blicamente o en privado), debe estar firmado:

```bash
# Firmar plugin (solo para distribuci√≥n)
npm run sign
```

> ‚ùó **Nota**: No es necesario firmar durante el desarrollo local. El entorno Docker permite ejecutarlo sin firma.

---

## ü§ù Casos de uso

### Monitoreo de infraestructura
- Detectar picos de CPU/memoria autom√°ticamente
- Explicar correlaciones entre m√©tricas
- Sugerir optimizaciones basadas en patrones

### An√°lisis financiero  
- Interpretar movimientos de acciones
- Detectar anomal√≠as en trading
- Comparar performance hist√≥rica

### Business Intelligence
- Explicar KPIs a stakeholders no t√©cnicos
- Identificar tendencias en ventas
- Alertas inteligentes sobre m√©tricas cr√≠ticas

---

## üîß Troubleshooting

### Plugin no aparece en Grafana
```bash
# Verificar que el build fue exitoso
npm run build

# Reiniciar containers
docker-compose restart
```

### Ollama no responde
```bash
# Verificar que Ollama est√° corriendo
ollama list

# Reiniciar servicio
ollama serve
```

### Problemas de permisos
```bash
sudo chown -R $USER:$USER .
```

---

## üöÄ Pr√≥ximos pasos

- Experiment√° con diferentes modelos de LLM seg√∫n tu hardware
- Personaliz√° los prompts para casos de uso espec√≠ficos
- Integr√° con tus propias fuentes de datos
- Expand√≠ los tipos de an√°lisis disponibles

**¬øNecesit√°s ayuda implementando esto en tu organizaci√≥n?** No dudes en contactarnos.

---

*Hacemos IA aplicada con sentido para crear soluciones con criterio.*
